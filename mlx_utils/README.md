# MLX Utils - HRM Implementation for Apple Silicon

This directory contains the complete MLX implementation of the Hierarchical Reasoning Model (HRM).

## ğŸ“ File Structure

```
mlx_utils/
â”œâ”€â”€ __init__.py          # Package initialization
â”œâ”€â”€ README.md            # This file
â”‚
â”œâ”€â”€ layers.py            # Core neural network layers
â”‚   â”œâ”€â”€ MLXAttention     # Replaces FlashAttention
â”‚   â”œâ”€â”€ MLXSwiGLU        # Activation function
â”‚   â”œâ”€â”€ MLXRotaryEmbedding  # Position encoding
â”‚   â”œâ”€â”€ rms_norm         # Normalization
â”‚   â””â”€â”€ MLXTransformerBlock  # Complete block
â”‚
â”œâ”€â”€ embeddings.py        # Embedding layers
â”‚   â”œâ”€â”€ MLXEmbedding     # Token embeddings
â”‚   â”œâ”€â”€ MLXSparseEmbedding   # Puzzle-specific embeddings
â”‚   â””â”€â”€ MLXCombinedEmbeddings  # Full input embeddings
â”‚
â”œâ”€â”€ hrm_model.py         # HRM architecture
â”‚   â”œâ”€â”€ HRMInnerCarry    # State management
â”‚   â”œâ”€â”€ MLXReasoningModule  # H/L level modules
â”‚   â”œâ”€â”€ MLXHRMInner      # Core reasoning engine
â”‚   â””â”€â”€ MLXHRM           # Complete model with ACT
â”‚
â”œâ”€â”€ losses.py            # Loss functions
â”‚   â”œâ”€â”€ stablemax_cross_entropy  # Numerically stable CE
â”‚   â”œâ”€â”€ softmax_cross_entropy    # Standard CE
â”‚   â””â”€â”€ MLXACTLossHead   # Complete loss computation
â”‚
â”œâ”€â”€ optimizers.py        # Optimization algorithms
â”‚   â”œâ”€â”€ SignSGD_MLX      # For sparse embeddings
â”‚   â”œâ”€â”€ AdamMLX          # Standard Adam
â”‚   â””â”€â”€ create_optimizer_for_hrm  # Optimizer factory
â”‚
â”œâ”€â”€ dataset.py           # Data loading
â”‚   â”œâ”€â”€ MLXPuzzleDataset # Main dataset class
â”‚   â”œâ”€â”€ DatasetMetadata  # Metadata structure
â”‚   â””â”€â”€ collate_batch    # Batching utilities
â”‚
â”œâ”€â”€ training.py          # Training loop
â”‚   â”œâ”€â”€ train_step       # Single training step
â”‚   â”œâ”€â”€ evaluate         # Evaluation logic
â”‚   â”œâ”€â”€ cosine_schedule_with_warmup  # LR scheduling
â”‚   â”œâ”€â”€ save/load_checkpoint  # Checkpointing
â”‚   â””â”€â”€ train            # Main training loop
â”‚
â””â”€â”€ utils.py             # Helper utilities
    â”œâ”€â”€ set_random_seed  # Reproducibility
    â”œâ”€â”€ count_parameters # Model info
    â”œâ”€â”€ Timer            # Profiling
    â”œâ”€â”€ tree_map         # PyTree utilities
    â””â”€â”€ check_nan_inf    # Debugging
```

## ğŸš€ Implementation Status

| Module | Status | Priority | Notes |
|--------|--------|----------|-------|
| `layers.py` | ğŸ“ Scaffolded | HIGH | Core building blocks |
| `embeddings.py` | ğŸ“ Scaffolded | HIGH | Input representations |
| `hrm_model.py` | ğŸ“ Scaffolded | HIGH | Main architecture |
| `losses.py` | ğŸ“ Scaffolded | MEDIUM | Loss computation |
| `optimizers.py` | ğŸ“ Scaffolded | MEDIUM | Training algorithms |
| `dataset.py` | ğŸ“ Scaffolded | MEDIUM | Data loading |
| `training.py` | ğŸ“ Scaffolded | HIGH | Training loop |
| `utils.py` | ğŸ“ Scaffolded | LOW | Helpers |

**Legend:**
- ğŸ“ Scaffolded: Interface defined, needs implementation
- ğŸš§ In Progress: Partially implemented
- âœ… Complete: Fully implemented and tested
- âœ“ Tested: Passing all tests

## ğŸ“Š Implementation Order

### Phase 1: Core Layers (Week 1)
1. `rms_norm` in `layers.py`
2. `MLXLinear` in `layers.py`
3. `MLXSwiGLU` in `layers.py`
4. `MLXRotaryEmbedding` in `layers.py`
5. `MLXAttention` in `layers.py`
6. `MLXTransformerBlock` in `layers.py`

**Test:** Single transformer block forward pass

### Phase 2: Embeddings (Week 1)
1. `trunc_normal_init` in `embeddings.py`
2. `MLXEmbedding` in `embeddings.py`
3. `MLXLearnedPositionEmbedding` in `embeddings.py`
4. `MLXSparseEmbedding` in `embeddings.py`
5. `MLXCombinedEmbeddings` in `embeddings.py`

**Test:** Input embedding generation

### Phase 3: HRM Model (Week 2)
1. `HRMInnerCarry`, `HRMCarry` in `hrm_model.py`
2. `MLXReasoningModule` in `hrm_model.py`
3. `MLXHRMInner` in `hrm_model.py`
4. `MLXHRM` in `hrm_model.py`

**Test:** Full model forward pass (single step)

### Phase 4: Losses & Optimizers (Week 2)
1. `stablemax`, `log_stablemax` in `losses.py`
2. `stablemax_cross_entropy` in `losses.py`
3. `MLXACTLossHead` in `losses.py`
4. `SignSGD_MLX` in `optimizers.py`
5. `create_optimizer_for_hrm` in `optimizers.py`

**Test:** Loss computation and gradient flow

### Phase 5: Data & Training (Week 3)
1. `DatasetMetadata`, data loading in `dataset.py`
2. `MLXPuzzleDataset` in `dataset.py`
3. `train_step` in `training.py`
4. `evaluate` in `training.py`
5. `train` in `training.py`
6. Utilities in `utils.py`

**Test:** Full training loop on small dataset

## ğŸ¯ Key Design Decisions

### 1. No Distributed Training
**Rationale:** MLX targets single Apple Silicon chips with unified memory.  
**Impact:** ~50% less code complexity, simpler API, easier debugging.

### 2. Native Attention (No FlashAttention)
**Rationale:** FlashAttention is CUDA-specific. MLX attention is Metal-optimized.  
**Impact:** Similar or better performance on Apple GPUs.

### 3. Simplified Sparse Embeddings
**Rationale:** MLX's unified memory eliminates need for complex buffer management.  
**Impact:** Cleaner implementation, easier to understand and maintain.

### 4. Single Optimizer API
**Rationale:** MLX optimizers work differently than PyTorch's.  
**Impact:** Use `apply_single` method instead of separate step() calls.

### 5. Lazy Evaluation
**Rationale:** MLX evaluates lazily for efficiency.  
**Impact:** Explicit `mx.eval()` calls needed in training loop.

## ğŸ”§ MLX-Specific Features

### Compilation
```python
@mx.compile
def compiled_attention(q, k, v):
    return scaled_dot_product_attention(q, k, v)
```

### Gradient Computation
```python
def loss_fn(model, batch):
    output, metrics = model(batch)
    return output["loss"], metrics

(loss, metrics), grads = mx.value_and_grad(loss_fn, has_aux=True)(model, batch)
```

### Unified Memory
```python
# No .cuda() or .to(device) needed!
# Data already accessible by GPU
batch = load_batch()
output = model(batch)  # Just works!
```

## ğŸ“ Coding Conventions

1. **Type hints:** Use typing annotations for all functions
2. **Docstrings:** Google-style docstrings with Args/Returns
3. **Comments:** Explain "why", not "what"
4. **Naming:** 
   - Classes: `PascalCase` with `MLX` prefix
   - Functions: `snake_case`
   - Constants: `UPPER_SNAKE_CASE`
5. **Structure:** Group related functions, clear section headers

## ğŸ§ª Testing Strategy

Each module should have corresponding tests:
```
tests/
â”œâ”€â”€ test_layers.py
â”œâ”€â”€ test_embeddings.py
â”œâ”€â”€ test_hrm_model.py
â”œâ”€â”€ test_losses.py
â”œâ”€â”€ test_optimizers.py
â”œâ”€â”€ test_dataset.py
â””â”€â”€ test_training.py
```

Test types:
1. **Unit tests:** Individual functions/classes
2. **Integration tests:** Multiple components together
3. **Gradient tests:** Verify gradients flow correctly
4. **Numerical tests:** Compare with PyTorch version
5. **Performance tests:** Benchmark speed and memory

## ğŸ“š Resources

- [MLX Documentation](https://ml-explore.github.io/mlx/)
- [MLX Examples](https://github.com/ml-explore/mlx-examples)
- [Original HRM Paper](https://arxiv.org/abs/2506.21734)
- [Original PyTorch Code](../models/)

## ğŸ› Known Limitations

1. **No multi-GPU:** MLX is single-device only
2. **macOS only:** Requires Apple Silicon (M1/M2/M3/M4)
3. **Python 3.9+:** MLX requires recent Python
4. **Memory:** Unified memory shared with system (no dedicated VRAM)

## ğŸ’¡ Optimization Tips

1. Use `@mx.compile` for hot paths
2. Batch operations where possible
3. Minimize Python loops over sequences
4. Profile with Metal System Trace
5. Use `mx.eval()` at checkpoints to free graph memory
6. Consider bf16 for speed (automatic in MLX)

## ğŸ¤ Contributing

When implementing a module:
1. Follow the scaffolded interface
2. Add comprehensive docstrings
3. Include type hints
4. Write unit tests
5. Verify gradients flow correctly
6. Benchmark against PyTorch version

## ğŸ“§ Contact

Questions? Check the parent README or open an issue.

---

**Status:** ğŸ“ Scaffolding Complete  
**Next:** Begin Phase 1 - Core Layers  
**Target:** Full implementation by Week 3

